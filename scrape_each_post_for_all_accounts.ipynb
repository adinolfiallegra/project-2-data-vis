{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import dependencies\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import json\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "\n",
    "from bs4 import BeautifulSoup as bs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Here are the account names\n",
    "accounts_list = [\n",
    "    'burberry',\n",
    "    'gucci',\n",
    "    'toryburch',\n",
    "    'michaelkors',\n",
    "    'bananarepublic',\n",
    "    'majeofficiel',\n",
    "    'aliceandolivia',\n",
    "    'coach',\n",
    "    'ferragamo',\n",
    "    'chloe'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Iterate through each account and let the code do its thang\n",
    "\n",
    "# Use this to check whether or not the page has refreshed\n",
    "# in case of running into rate-limiting issues\n",
    "refresh_check = 'initial string, does not matter'\n",
    "\n",
    "for account_name in accounts_list:\n",
    "    \n",
    "    # Load the json file to read the urls list\n",
    "    account_data_dict = json.load(open(f'data/{account_name}.json'))\n",
    "    post_urls_list = account_data_dict['post_urls_list']\n",
    "\n",
    "    # Initiate a list to keep the post data\n",
    "    post_data_list = []\n",
    "\n",
    "    #Initiate browser\n",
    "    driver = webdriver.Chrome()\n",
    "\n",
    "    # Loop through the posts and scrape data\n",
    "    for post_url in post_urls_list:\n",
    "\n",
    "        # Visit post\n",
    "        driver.get(post_url)\n",
    "\n",
    "        # Fetch like or view count\n",
    "        ## Keep placeholders for like and view counts\n",
    "        like_count = None\n",
    "        view_count = None\n",
    "\n",
    "        try:\n",
    "            # If the page has a '_nzn1h' class div, then it's a photo\n",
    "            ## Find the tag that contains the like count string\n",
    "            like_count_str = driver.find_elements_by_class_name('_nzn1h')[0].text\n",
    "            ## Parse the string to get a numeric like count\n",
    "            like_count = int(like_count_str.split(' ')[0].replace(',', ''))\n",
    "        except:\n",
    "            # If it doesn't, then it's a video\n",
    "            ## Find the tag that contains the view count string\n",
    "            view_count_str = driver.find_elements_by_class_name('_m5zti')[0].text\n",
    "            ## Parse the string to get a numeric view count\n",
    "            view_count = int(view_count_str.split(' ')[0].replace(',', ''))\n",
    "\n",
    "        # See if the post is a album, by looking for the next button\n",
    "        next_button_list = driver.find_elements_by_class_name('coreSpriteRightChevron')\n",
    "\n",
    "        if len(next_button_list) > 0:   # Check for next button\n",
    "            post_type = 'album'\n",
    "        elif like_count in locals():   # Check for like count\n",
    "            post_type = 'photo'\n",
    "        else:\n",
    "            post_type = 'video'\n",
    "\n",
    "        # Fetch post datetime\n",
    "        ## Use BeautifulSoup to find the hidden datetime attribute\n",
    "        post_time_broth = driver.find_elements_by_class_name('_djdmk')[0].get_attribute('innerHTML')\n",
    "        post_time_soup = bs(post_time_broth, 'html.parser')\n",
    "        post_datetime_str = post_time_soup.find('time')['datetime']\n",
    "\n",
    "        # Record the caption word count and number of hastags/@s\n",
    "        ## Fetch everything in the caption area\n",
    "        caption_str = driver.find_elements_by_class_name('_ezgzd')[0].text\n",
    "        ## Split the whole string into a list of words\n",
    "        caption_word_list = caption_str.replace('\\n', ' ').replace('.', '')\\\n",
    "        .replace(',', '').replace('!', '').replace('?', '').split(' ')\n",
    "        caption_length = len(caption_word_list)\n",
    "        ## Derive the hashtag and @ data\n",
    "        has_hashtag = False\n",
    "        hashtag_count = 0\n",
    "\n",
    "        has_at = False\n",
    "        at_count = 0\n",
    "\n",
    "        ## Iterate through the caption words to count hashtags and @s\n",
    "        for each in caption_word_list:\n",
    "            if '#' in each:\n",
    "                hashtag_count += 1\n",
    "            elif '@' in each:\n",
    "                at_count += 1\n",
    "            else:\n",
    "                pass\n",
    "\n",
    "        ## Record whether or the post has hashtags and @s\n",
    "        if hashtag_count > 0:\n",
    "            has_hashtag = True\n",
    "\n",
    "        if at_count > 0:\n",
    "            has_at = True\n",
    "\n",
    "        # Construct the dictionary to hold post data\n",
    "        post_data_dict = {\n",
    "            'post_url': post_url,\n",
    "            'like_count': like_count,\n",
    "            'view_count': view_count,\n",
    "            'post_type': post_type,\n",
    "            'post_datetime_str': post_datetime_str,\n",
    "            'has_hashtag': has_hashtag,\n",
    "            'hashtag_count': hashtag_count,\n",
    "            'has_at': has_at,\n",
    "            'at_count': at_count\n",
    "        }\n",
    "\n",
    "        # Update post data list\n",
    "        # but only if it's not a repeat.\n",
    "        # A repeat would indicate the new page did not load,\n",
    "        # meaning that we're being rate-limited\n",
    "        if post_data_dict not in post_data_list:\n",
    "            post_data_list.append(post_data_dict)\n",
    "        # Wait for 10 minutes if rate-limited\n",
    "        else:\n",
    "            time.sleep(60 * 10)\n",
    "\n",
    "        # Keep track of progress\n",
    "        already_recorded = len(post_data_list)\n",
    "        total_to_record = len(post_urls_list)\n",
    "        time_check = datetime.strftime(datetime.now(), '%I: %M: %S.%f')\n",
    "        print(f'{account_name} -- {time_check}: {already_recorded} of {total_to_record}')\n",
    "\n",
    "    # CLose the browser at the end\n",
    "    driver.close()\n",
    "\n",
    "    # Append account data dictionary and save to new json file\n",
    "    account_data_dict = json.load(open(f'data/{account_name}.json'))\n",
    "    account_data_dict['post_data_list'] = post_data_list\n",
    "\n",
    "    with open(f'data/appended/appended_{account_name}.json', 'w') as file:\n",
    "            json.dump(account_data_dict, file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
